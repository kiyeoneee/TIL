# 4. 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포

## 파드를 안정적으로 유지하기

### Liveness probe

> 컨테이너가 살아있는지를 확인하는 서비스
> 쿠버네티스는 주기적으로 probe를 실행하고 실패할 경우 컨테이너를 다시 시작
> 파드를 호스팅하는 노드의 Kubelet에서 수행

#### 메커니즘

- HTTP GET probe
  - 지정한 IP 주소, port, 경로에 GET 요청하여 응답을 수신하고, HTTP response 코드가 오류가 아닌 경우 성공으로 간주
    이외에는 실패로 간주돼 컨테이너를 다시 시작
- TCP 소켓 probe
  - 컨테이너의 지정된 포트에 TCP 연결 시도하여 연결에 성공하면 프로브 성공, 연결 실패 시 컨테이너 재시작
- Exec probe
  -  컨테이너 내의 임의의 명령을 실행하고, 종료 상태 코드를 확인하여 0인 경우 프로브 성공 이외에는 실패로 간주
    JVM 기반의 애플리케이션과 같이 기동 절차에 큰 연산 리소스가 필요한 경우 Exec 보다는 HTTP Get 프로브 사용 추천

```yaml
$ cat kubia-with-error-for-probe.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 15

$ kubectl apply -f kubia-with-error-for-probe.yaml
pod/kubia-liveness created
$ kubectl get po kubia-liveness
NAME             READY   STATUS    RESTARTS   AGE
kubia-liveness   1/1     Running   0          4s
$ kubectl describe po kubia-liveness
Name:         kubia-liveness
Namespace:    default
Priority:     0
Node:         minikube/192.168.49.2
Start Time:   Thu, 18 Feb 2021 00:08:05 +0900
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           172.17.0.3
IPs:
  IP:  172.17.0.3
Containers:
  kubia:
    Container ID:   docker://5f27ae9125b054d1f5ecb099d92c44b2902424b57e1ccc19a963f2079f59ccf5
    Image:          luksa/kubia-unhealthy
    Image ID:       docker-pullable://luksa/kubia-unhealthy@sha256:5c746a42612be61209417d913030d97555cff0b8225092908c57634ad7c235f7
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 18 Feb 2021 00:08:08 +0900
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:8080/ delay=15s timeout=1s period=10s #success=1 #failure=3
  ...
$ kubectl get po kubia-liveness
NAME             READY   STATUS             RESTARTS   AGE
kubia-liveness   0/1     CrashLoopBackOff   7          20m
$ kubectl describe po kubia-liveness
Name:         kubia-liveness
Namespace:    default
Priority:     0
Node:         minikube/192.168.49.2
Start Time:   Thu, 18 Feb 2021 00:08:05 +0900
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           172.17.0.3
IPs:
  IP:  172.17.0.3
Containers:
  kubia:
    Container ID:   docker://8122323b9af31ef1ce8c91bc6aa0b84623bea3e608298a035ca9be421136004e
    Image:          luksa/kubia-unhealthy
    Image ID:       docker-pullable://luksa/kubia-unhealthy@sha256:5c746a42612be61209417d913030d97555cff0b8225092908c57634ad7c235f7
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 18 Feb 2021 00:24:59 +0900
      Finished:     Thu, 18 Feb 2021 00:26:55 +0900
    Ready:          False
    Restart Count:  7
    Liveness:       http-get http://:8080/ delay=15s timeout=1s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wtnhv (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
...


```

### 프로브에 재시도 루프를 구현하지 마라

프로브 실패 임곗값을 설정할 수 있으나 쿠버네티스는 한 번의 실패를 인지하기 전에 여러 번 재시도
실패 임곗값을 1로 지정해도 의미없으로 별도의 구현은 지양할 것

<br>

## 레플리케이션컨트롤러 소개

> 파드가 항상 실행되도록 보장
> 노드 장애로 파드의 유실 여부를 인지하면 새로운 파드 인스턴스를 생성

### 레플리케이션컨트롤러의 동작

- 실행 중인 파드 목록을 지속적으로 모니터링
- 레이블 셀렉터와 매치되는 파드의 실제 파드 수 == 설정된 파드 수 항상 확인
- 일치하지 않을 시 파드를 늘리거나 줄임

#### 레플리케이션 컨트롤러의 세가지 요소

- 레이블 셀렉터 : 레플리케이션 컨트롤러의 범위에 있는 파드를 결정
- 레플리카 수 : 실행할 파드의 의도하는 수를 저장
- 파드 템플릿 : 새로운 파드 레플리카를 만들 떄 사용

레이블 셀렉터 변경 -> 기존 파드가 레플리케이션컨트롤러의 범위를 벗어나므로 컨트롤러가 해당 파드에 대한 관리 중지
템플릿 -> 이 레플리케이션컨트롤러로 새 파드로 생성할 때만 영향을 미침

### 이점

- 기존 파드가 사라지면 새 파드를 시작해 파드가 항상 실행되도록 함
- 클러스터 노드에 장애 발생시 모든 파드에 관한 교체 복제본이 생성
- 수동 또는 자동으로 파드를 쉽게 수평으로 확장 가능

```bash
$ cat kubia-rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080

$ kubectl create -f kubia-rc.yaml
replicationcontroller/kubia created
$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-bdh89   1/1     Running   0          4m45s
kubia-tmvfr   1/1     Running   0          4m45s
kubia-vgd2x   1/1     Running   0          4m45s
$ kubectl delete pod kubia-bdh89
pod "kubia-bdh89" deleted
$ kubectl get pods
NAME          READY   STATUS        RESTARTS   AGE
kubia-9h6fw   1/1     Running       0          11s    # 아래의 bdh89 파드가 삭제되자 RC가 바로 사로운 파드 생성
kubia-bdh89   1/1     Terminating   0          7m55s
kubia-tmvfr   1/1     Running       0          7m55s
kubia-vgd2x   1/1     Running       0          7m55s
```

<br>

## 레플리케이션컨트롤러 대신 레플리카셋 사용하기

> 레플리케이션컨트롤러가  deprecated 된 후 레플리카셋으로 완전히 대체

### 레플리카셋과 레플리케이션컨트롤러 비교

- 레플리케이션컨트롤러와 동일하게 동작하지만 파드 셀렉터가 더 풍부한 표현식을 사용
- v1 API 그룹에 포함되어 있지 않고 apps API 그룹에 포함되어 있음

```bash
$ cat kubia-replicaset.yaml
apiVersion: apps/v1  # 202102 기준, 책에는 apps/v1beta2 공식 문서에는 apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080

$ kubectl create -f kubia-replicaset.yaml
replicaset.apps/kubia created
$ kubectl get rs
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         2       7s
$ kubectl get rs
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         2       7s
➜  kubia kubectl describe rs
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age    From                   Message
  ----    ------            ----   ----                   -------
  Normal  SuccessfulCreate  2m35s  replicaset-controller  Created pod: kubia-xsvmn
  Normal  SuccessfulCreate  2m35s  replicaset-controller  Created pod: kubia-9j59s
  Normal  SuccessfulCreate  2m35s  replicaset-controller  Created pod: kubia-rr7xq
```

#### 레플리카셋의 더욱 표현적인 레이블 셀렉터 사용하기

**MatchExpression**

```bash
$ cat kubia-replicaset-matchexp.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia-matchexp
spec:
  replicas: 3
  selector:
    #matchLabels:
      #apps: kubia
    matchExpressions:
      - key: app
        operator: In
        values:
          - kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080

$ kubectl create -f kubia-replicaset-matchexp.yaml
replicaset.apps/kubia-matchexp created
$ kubectl get rs
NAME             DESIRED   CURRENT   READY   AGE
kubia            3         3         3       14m
kubia-matchexp   3         3         1       7s
```

<br>

## 데몬셋을 사용해 각 노드에서 정확히 한 개의 파드 실행하기

- 레플리케이션 컨트롤러와 레플리카셋 : 쿠버네티스 클러스 내 무작위로 지정된 수만큼의 파드를 실행
- 데몬셋 : 모든 노드에 하나의 파드를 실행

#### 데몬셋으로 모든 노드에 파드 실행하기

데몬셋에 의해 생성되는 파드는 타깃 노드가 이미 지정돼 있고 쿠버네티스 스케줄러를 건너뜀
-> 노드의 레이블이 변경되면 타깃 노드에서 제외되므로 자동으로 파드가 삭제됨

```bash
$ cat ssd-monitor-daemonset.yml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor

$ kubectl apply -f ssd-monitor-daemonset.yml
daemonset.apps/ssd-monitor created
$ kubectl get pods
NAME                READY   STATUS    RESTARTS   AGE
ssd-monitor-7jll9   1/1     Running   0          5s
$ kubectl get nodes
NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   12d   v1.20.0
```

<br>

## 완료 가능한 단일 태스크를 수행하는 파드 실행

### Job

> 지속적인 태스크를 관리하는 레플리케이션컨트롤러, 레플리카셋, 데몬셋과 달리 완료 가능한 태스크를 관리

```bash
# 기본적인 잡 생성
$ cat exporter.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job

$ kubectl create -f exporter.yaml
job.batch/batch-job created
$  kubectl get po
NAME                   READY   STATUS    RESTARTS   AGE
batch-job-jmrx6        1/1     Running   0          91s
$ kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   1/1           2m5s       2m41s
$ kubectl get po  # 2분이 지난 후 배치가 끝나 pod이 종료됨
NAME                   READY   STATUS        RESTARTS   AGE
batch-job-jmrx6        0/1     Completed     0          2m42s
$ kubectl logs batch-job-jmrx6
Thu Feb 18 03:11:42 UTC 2021 Batch job starting
Thu Feb 18 03:13:42 UTC 2021 Finished succesfully
```

### 잡에서 여러 파드 인스턴스 실행하기

```bash
$ kubectl get jobs
NAME                         COMPLETIONS   DURATION   AGE
batch-job                    1/1           2m5s       9m3s
multi-completion-batch-job   0/5           6s         6s
$ kubectl get jobs
NAME                         COMPLETIONS   DURATION   AGE
batch-job                    1/1           2m5s       9m16s
multi-completion-batch-job   0/5           19s        19s
$ kubectl get po
NAME                               READY   STATUS      RESTARTS   AGE
batch-job-jmrx6                    0/1     Completed   0          9m8s
kubia-9h6fw                        1/1     Running     0          3h15m
multi-completion-batch-job-fwj9t   1/1     Running     0          12s
$ kubectl scale job multi-completion-batch-job --replicas 3
Error from server (NotFound): the server could not find the requested resource

# 병렬로 여러 잡 파드 실행
$ cat multi-completion-parallel-batch-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5
  parallelism: 2
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job

$ kubectl apply -f multi-completion-parallel-batch-job.yaml
job.batch/multi-completion-batch-job created
$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
multi-completion-batch-job-2mw8k   0/1     ContainerCreating   0          3s
multi-completion-batch-job-mp54h   0/1     ContainerCreating   0          3s
$ kubectl get jobs
NAME                         COMPLETIONS   DURATION   AGE
multi-completion-batch-job   0/5           6s         6s
$ kubectl scale job multi-completion-batch-job --replicas 3
Error from server (NotFound): the server could not find the requested resource
```

<br>

## 잡을 주기적으로 또는 한 번 실행되도록 스케줄링하기